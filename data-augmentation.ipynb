{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Cassavaのリベンジ用"},{"metadata":{"trusted":true},"cell_type":"code","source":"# efficientnetのロード\n# !pip install --quiet efficientnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ライブラリーのロード\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom kaggle_datasets import KaggleDatasets\nfrom albumentations import (\n    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n    Rotate, Normalize\n)\nimport albumentations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TPU or GPUの設定"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPU or GPU setting\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_CLASSES = 5\nIMAGE_SIZE = [512, 512]\nBATCH_SIZE = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CFG[\"N_CLASSES\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## データのロードセクション"},{"metadata":{"trusted":true},"cell_type":"code","source":"# データのパスを指定する(tfrecordにすること)\nGCS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-tfrecords-512x512')\nFILENAME_COMP = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 数える関数\ndef count_data_items(filename):\n    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for name in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affine変換用の行列\n# original(https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96)\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affine transform\n# original(https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96)\n# 各種パラメータはalbumentationに合わせたはず...\ndef affine_transform(image):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    # rot = 15. * tf.random.normal([1],dtype='float32')\n    rot = 45. * tf.random.uniform([1], -1.0, 1.0, dtype='float32')\n    # shr = 5. * tf.random.normal([1],dtype='float32') \n    shr = 0. * tf.random.normal([1],dtype='float32') \n    # h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    # w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    # h_shift = 16. * tf.random.normal([1],dtype='float32') \n    h_shift = 0.0625 * tf.random.uniform([1], -1.0, 1.0, dtype='float32') \n    # w_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 0.0625 * tf.random.uniform([1], -1.0, 1.0, dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(image, label, transpose_arg_p = 0, random_crop_arg_p = 0, \n                 horizontal_flip_arg_p = 0.5, vertical_flip_arg_p = 0.5, \n                 affine_transform_arg_p = 0.5\n                ):\n    p_random_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_transpose = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_vertical_flip = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_horizontal_flip = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_affine_transform= tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    if p_random_crop <= random_crop_arg_p:\n        image = tf.image.random_crop(image, size=[IMAGE_SIZE[0], IMAGE_SIZE[1], 3])\n    \n    # transpose\n    if p_transpose <= transpose_arg_p:\n        image = tf.transpose(image, perm=[1, 0, 2])\n    \n    # vertical_flip\n    if p_vertical_flip <= vertical_flip_arg_p:\n        image = tf.image.flip_left_right(image)\n    \n    # horizontal_flip\n    if p_vertical_flip <= vertical_flip_arg_p:\n        image = tf.image.flip_up_down(image)\n    \n    # affine_transform\n    if p_affine_transform <= affine_transform_arg_p:\n        image = affine_transform(image)\n    \n\n    \n    return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alb_transforms = Compose([\n    # albumentations.Cutout(max_h_size=int(IMAGE_SIZE[0] * 0.3), max_w_size=int(IMAGE_SIZE[1] * 0.3), num_holes=1, p=0.5), \n    # RandomBrightness(p=),\n    Normalize(), \n])\ndef aug_fn(image):\n    data ={\"image\":image}\n    aug_data = alb_transforms(**data)\n    aug_img = aug_data[\"image\"]\n    # aug_img = tf.cast(aug_img/255.0, tf.float32)\n    return aug_img\n\ndef alb_process_data(image, label):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n    return aug_img, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    \"\"\"\n        Decode a JPEG-encoded image to a uint8 tensor.\n    \"\"\"\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    return image\n\n# フロートに変換することだけに注意\ndef scale_image(image, label):\n    \"\"\"\n        Cast tensor to float and normalizes (range between 0 and 1).\n    \"\"\"\n    image = tf.cast(image, tf.float32)\n    image /= 255.0\n    return image, label\n\ndef image_to_float(image, label):\n    image = tf.cast(image, tf.float32)\n    return image, label\n\n\ndef read_tfrecord(example, labeled=True):\n    \"\"\"\n        1. Parse data based on the 'TFREC_FORMAT' map.\n        2. Decode image.\n        3. If 'labeled' returns (image, label) if not (image, name).\n    \"\"\"\n    if labeled:\n        TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string), \n            'target': tf.io.FixedLenFeature([], tf.int64), \n        }\n    else:\n        TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string), \n            'image_name': tf.io.FixedLenFeature([], tf.string), \n        }\n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    image = decode_image(example['image'])\n    if labeled:\n        label_or_name = tf.cast(example['target'], tf.int32)\n        # One-Hot Encoding needed to use \"categorical_crossentropy\" loss\n        label_or_name = tf.one_hot(tf.cast(label_or_name, tf.int32), N_CLASSES)\n    else:\n        label_or_name = example['image_name']\n    return image, label_or_name\n\n\ndef get_dataset(FILENAMES, labeled=True, ordered=False, cached=False, augment=False):\n    \n    ignore_order = tf.data.Options()\n    \n    if not ordered:\n        ignore_order.experimental_deterministic = False\n        dataset = tf.data.Dataset.list_files(FILENAMES)\n        dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n    else:\n        dataset = tf.data.TFRecordDataset(FILENAMES, num_parallel_reads=AUTO)\n    \n    \n    dataset = dataset.with_options(ignore_order)\n    # データの読み込み\n    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled), num_parallel_calls=AUTO)\n    \n    # data augmentation\n    if augment:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n        # albumentationを使うためにfloatに変換する\n        dataset = dataset.map(image_to_float, num_parallel_calls=AUTO)\n        dataset = dataset.map(alb_process_data, num_parallel_calls=AUTO)\n    else:\n        dataset = dataset.map(scale_image, num_parallel_calls=AUTO)  \n    dataset = dataset.batch(BATCH_SIZE) \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## データセット表示用"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_dataset(FILENAME_COMP, ordered=True)\ndataset = train_dataset.unbatch()\nfor img, label in dataset:\n    plt.imshow(img.numpy())\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_dataset(FILENAME_COMP, ordered=True, augment=True)\ndataset = train_dataset.unbatch()\nfor img, label in dataset:\n    plt.imshow(img.numpy())\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}